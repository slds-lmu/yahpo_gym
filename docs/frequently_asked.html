

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Frequently Asked Questions &mdash; yahpo_gym 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=2389946f"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Scenarios &amp; Instances" href="scenarios.html" />
    <link rel="prev" title="Getting Started" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            yahpo_gym
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="yahpo_gym.html">YAHPO Gym Module Handbook</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#citation">Citation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#openml-task-id-and-dataset-id">OpenML task_id and dataset_id</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reproducibility">Reproducibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lcbench-and-epochs">lcbench and epochs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#monotonicity-in-runtime">Monotonicity in Runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-f1-scores-for-rbv2">Using F1 scores for rbv2_*</a></li>
<li class="toctree-l2"><a class="reference internal" href="#memory-estimation-for-rbv2">Memory Estimation for rbv2_*</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multi-objective-benchmarking">Multi-Objective Benchmarking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performance-metrics-for-rbv2-xgboost">Performance Metrics for rbv2_xgboost</a></li>
<li class="toctree-l2"><a class="reference internal" href="#noisy-surrogates">Noisy Surrogates</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="scenarios.html">Scenarios &amp; Instances</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples Gallery</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending YAHPO Gym</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">yahpo_gym</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Frequently Asked Questions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/frequently_asked.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="frequently-asked-questions">
<h1>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this heading"></a></h1>
<p>In the following, we maintain a list of frequently asked questions.</p>
<section id="citation">
<h2>Citation<a class="headerlink" href="#citation" title="Permalink to this heading"></a></h2>
<p>If you use YAHPO Gym, please cite the following paper:</p>
<ul class="simple">
<li><p>Pfisterer, F., Schneider, L., Moosbauer, J., Binder, M., &amp; Bischl, B. (2022). YAHPO Gym - An Efficient Multi-Objective Multi-Fidelity Benchmark for Hyperparameter Optimization. In International Conference on Automated Machine Learning.</p></li>
</ul>
<p>Moreover, certain <cite>scenarios</cite> built upon previous work, e.g., the <cite>lcbench</cite> scenario uses data from:</p>
<ul class="simple">
<li><p>Zimmer, L., Lindauer, M., &amp; Hutter, F. (2021). Auto-Pytorch: Multi-Fidelity Metalearning for Efficient and Robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(9), 3079-3090.</p></li>
<li><p>Zimmer, L. (2020). data_2k_lw.zip. figshare. Dataset. <a class="reference external" href="https://doi.org/10.6084/m9.figshare.11662422.v1">https://doi.org/10.6084/m9.figshare.11662422.v1</a>, Apache License, Version 2.0.</p></li>
</ul>
</section>
<section id="openml-task-id-and-dataset-id">
<h2>OpenML task_id and dataset_id<a class="headerlink" href="#openml-task-id-and-dataset-id" title="Permalink to this heading"></a></h2>
<p>Currently, the <cite>rbv2_*</cite>, <cite>lcbench</cite>, and <cite>iaml_*</cite> scenarios contain instances based on OpenML datasets.
For <cite>rbv2_*</cite> and <cite>iaml_*</cite> scenarios, the <cite>task_id</cite> parameter of the <cite>ConfigSpace</cite> corresponds to the OpenML <strong>dataset</strong> identifier (i.e., this is the <strong>dataset</strong> id and <strong>not</strong> the task id).
To query meta information, use <a class="reference external" href="https://www.openml.org/d">https://www.openml.org/d</a>/&lt;dataset_id&gt;.
For the <cite>lcbench</cite> scenario, the <cite>OpenML_task_id</cite> parameter of the <cite>ConfigSpace</cite> directly corresponds to OpenML <strong>tasks</strong> identifier (i.e., this is the <strong>task</strong> id and <strong>not</strong> the dataset id).
To query meta information, use <a class="reference external" href="https://www.openml.org/t">https://www.openml.org/t</a>/&lt;task_id&gt;.</p>
</section>
<section id="reproducibility">
<h2>Reproducibility<a class="headerlink" href="#reproducibility" title="Permalink to this heading"></a></h2>
<p><cite>YAHPO Gym</cite> relies on static neural networks compiled via <cite>ONNX</cite>.
This should result in reproducible results given equal hardware and software versions.
Unfortunately, <cite>ONNX</cite> models do not always yield reproducible results across different hardware.
This is, e.g. discussed in <a class="reference external" href="https://github.com/microsoft/onnxruntime/issues/12086">https://github.com/microsoft/onnxruntime/issues/12086</a>.</p>
<p>In practice, we have not observed relevant differences between different hardware versions, but this might help to explain observations
regarding a lack of exact reproducibility.</p>
</section>
<section id="lcbench-and-epochs">
<h2>lcbench and epochs<a class="headerlink" href="#lcbench-and-epochs" title="Permalink to this heading"></a></h2>
<p>The original LCBench data includes 52 epochs.
The surrogates of <cite>YAHPO Gym</cite> v1.0 were trained on this data.
Note, however, that the first epoch in the LCBench data refers to the models only being initialized (i.e., not trained).
Usually, it is therefore best to exclude this first epoch for learning curve purposes or when doing multi-fidelity HPO.
Moreover, the last epoch in the original LCBench data mostly contains exactly the same performance metrics as the penultimate epoch.
Often, a sensible epoch range for the <cite>lcbench</cite> scenario in <cite>YAHPO Gym</cite> is therefore given by 2 to 51.</p>
</section>
<section id="monotonicity-in-runtime">
<h2>Monotonicity in Runtime<a class="headerlink" href="#monotonicity-in-runtime" title="Permalink to this heading"></a></h2>
<p>Currently, <cite>YAHPO Gym</cite> surrogates do <strong>not</strong> enforce runtime predictions to be monotone increasing with respect to the fidelity parameter.
This is mainly due to most of our scenarios not involving the training of neural networks (except for <cite>nb301</cite> and <cite>lcbench</cite>) and in the case of, e.g., the fidelity parameter being <cite>trainsize</cite>, it is not necessary meaningful to assume a monotone increasing relationship between runtime and fidelity.
As we are using the same surrogate architecture for all scenarios, monotonicity is therefore also not enforced for the <cite>lcbench</cite> and <cite>nb301</cite> scenarios.
We plan to incorporate surrogates that enforce a monotone increasing relationship between runtime and and the fidelity parameter for the <cite>lcbench</cite> and <cite>nb301</cite> scenarios in upcoming versions of <cite>YAHPO Gym</cite>.</p>
</section>
<section id="using-f1-scores-for-rbv2">
<h2>Using F1 scores for rbv2_*<a class="headerlink" href="#using-f1-scores-for-rbv2" title="Permalink to this heading"></a></h2>
<p><cite>F1</cite> scores in the <cite>rbv2_*</cite> scenarios are only available for binary classification datasets.
On multiclass datasets, the corresponding <cite>F1</cite> score is imputed with <cite>0</cite> and returned by the surrogate model.
The information on which <cite>id</cite> corresponds to a multiclass dataset can be obtained from the entry <cite>is_multicrit</cite> in <cite>BenchmarkSet.config.config</cite>.</p>
</section>
<section id="memory-estimation-for-rbv2">
<h2>Memory Estimation for rbv2_*<a class="headerlink" href="#memory-estimation-for-rbv2" title="Permalink to this heading"></a></h2>
<p>For the <cite>rbv2_*</cite> settings, memory consumption was estimated by observing the memory consumption during training via <cite>/usr/bin/time</cite>.
This estimates the <cite>maximum resident size</cite>.
In general, we assume that this provides a coarse estimation of the processes memory consumption.
However, it does not seem to work if the goal, is to, e.g., measure memory consumption across <em>learning curves</em>.
In this setting, we often observe constant memory consumption across a full learning curve and also very low memory estimates close to <cite>0</cite>.
We therefore discourage using memory metrics in this setting.
In addition, memory estimation was not always logged properly resulting in memory consumption imputed with <cite>0</cite>, which might lead to problems on some instances.</p>
</section>
<section id="multi-objective-benchmarking">
<h2>Multi-Objective Benchmarking<a class="headerlink" href="#multi-objective-benchmarking" title="Permalink to this heading"></a></h2>
<p>We observed that one some multi-objective benchmark problems, Pareto fronts can collapse, i.e., although we initially
assume that objectives are in competition we can find a single best point that optimizes all objectives simultaneously
and optimizers can then proceed to only further optimize a subset of all objectives because the other ones have
become irrelevant.</p>
<p>While we believe that this is still a well defined multi-objective optimization problem and multi-objective quality
indicators can still be computed (even if the resulting Pareto set contains only a single point) we want to note that
such problems can introduce some biases, i.e., favouring optimizers that explore the extreme regions of the Pareto front.</p>
<p>This mostly affects <cite>rbv2_*</cite> scenarios (mostly <cite>rbv2_xgboost</cite> and <cite>rbv2_super</cite>) and hardware metrics like <cite>memory</cite> but
can sometimes also be observed for <cite>iaml_*</cite> scenarios (e.g., if <cite>nf</cite> is included as an objective).</p>
<p>For <cite>rbv2_*</cite> problems, this is a result of the memory estimation (see above), but in general, this effect is intensified
by the extrapolation behavior of the surrogate.</p>
<p>We will try to address this issue in upcoming versions of <cite>YAHPO Gym</cite>.</p>
</section>
<section id="performance-metrics-for-rbv2-xgboost">
<h2>Performance Metrics for rbv2_xgboost<a class="headerlink" href="#performance-metrics-for-rbv2-xgboost" title="Permalink to this heading"></a></h2>
<p>We observed that our surrogate for the <cite>rbv2_xgboost</cite> scenarios tends to predict very good performance (e.g., <cite>acc</cite>, <cite>auc</cite>) for most <cite>instances</cite> for a large amount of hyperparameter configurations.
While XGBoost can be considered state-of-the art on tabular data and very good performance can be expected, this might also be a result of an unaccounted ceiling effect within the surrogate.</p>
<p>We are looking into this issue and will try to address it in upcoming versions of <cite>YAHPO Gym</cite>.</p>
</section>
<section id="noisy-surrogates">
<h2>Noisy Surrogates<a class="headerlink" href="#noisy-surrogates" title="Permalink to this heading"></a></h2>
<p><cite>YAHPO Gym</cite> allows using <em>noisy</em> surrogates, this means that surrogates will predict targets from a distribution conditional on hyperparameters.
This internally works as follows:
1. Given 3 neural networks <cite>f_1</cite> - <cite>f_3</cite> that predict targets from hyperparameters, run the prediction step
2. Sample a vector alpha of length 3, such that each <cite>alpha_i</cite> is in <cite>[0, 1]</cite> and they sum to 1
3. The noisy prediction is given by the sum of neural network predictions weighted by the respective alpha</p>
<p>While this works well in theory, this was not tested thoroughly and the use of noisy surrogates is therefore discouraged at the moment.
Furthermore, we have not extensively tested whether all noisy surrogates indeed correctly return noisy predictions.
We will improve this in upcoming versions of <cite>YAHPO Gym</cite>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Getting Started" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="scenarios.html" class="btn btn-neutral float-right" title="Scenarios &amp; Instances" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Florian Pfisterer, Lennart Schneider.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>